{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os.path\n",
    "from os import path\n",
    "from IPython.display import display, clear_output\n",
    "from gym import wrappers\n",
    "\n",
    "class Memory:\n",
    "  def __init__(self, dim):\n",
    "    self.current = 0\n",
    "    self.has = 0\n",
    "    self.batchsize = 32\n",
    "    self.maxsize = 1000000\n",
    "\n",
    "    self.states = np.empty((self.maxsize, dim))\n",
    "    self.new_states = np.empty((self.maxsize, dim))\n",
    "    self.rewards = np.empty(self.maxsize)\n",
    "    self.actions = np.empty(self.maxsize, dtype=int)\n",
    "    self.dones = np.empty(self.maxsize)\n",
    "  \n",
    "  def remember(self, state, action, reward, new_state, done):\n",
    "    self.states[self.current] = state\n",
    "    self.new_states[self.current] = new_state\n",
    "    self.rewards[self.current] = reward\n",
    "    self.actions[self.current] = action\n",
    "    self.dones[self.current] = 1 if done else 0\n",
    "\n",
    "    self.has = min(self.has+1, self.maxsize)\n",
    "    self.current = (self.current+1)%self.maxsize\n",
    "  \n",
    "  def canLearn (self):\n",
    "  \tif self.has < self.batchsize:\n",
    "  \t\treturn False\n",
    "  \treturn True\n",
    "\n",
    "  def samples(self):\n",
    "    if self.has < self.batchsize:\n",
    "      return [[], [], [], [], []]\n",
    "    \n",
    "    indexes = np.random.randint(0, self.has, self.batchsize)\n",
    "    return [self.states[indexes], self.new_states[indexes], self.rewards[indexes], self.actions[indexes], self.dones[indexes]]\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = Memory(self.env.observation_space.shape[0])\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay_duration = 1e6\n",
    "        self.learning_rate = 0.0000625\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(128, input_dim=state_shape[0], activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling(scale=2), bias_initializer=keras.initializers.Zeros()))\n",
    "        model.add(Dense(128, activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling(scale=2), bias_initializer=keras.initializers.Zeros()))\n",
    "        model.add(Dense(128, activation=\"relu\", kernel_initializer=keras.initializers.VarianceScaling(scale=2), bias_initializer=keras.initializers.Zeros()))\n",
    "        model.add(Dense(self.env.action_space.n, kernel_initializer=keras.initializers.VarianceScaling(scale=2), bias_initializer=keras.initializers.Zeros()))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.remember(state, action, reward, new_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        states, new_states, rewards, actions, dones = self.memory.samples()\n",
    "\n",
    "        if len(states) == 0:\n",
    "          return\n",
    "        \n",
    "        arg_q_max = np.argmax(self.model.predict(new_states), axis=1)\n",
    "        q_vals = self.model.predict(states)\n",
    "        Q_futures = self.target_model.predict(new_states)[np.arange(0,32), arg_q_max]\n",
    "        q_vals[np.arange(0,32), actions] = rewards + Q_futures*self.gamma*(1-dones)\n",
    "        loss_hist = self.model.fit(states, q_vals, epochs=1, verbose=0)\n",
    "\n",
    "        return [loss_hist.history['loss'][0], np.mean(Q_futures), np.mean(q_vals)]\n",
    "    \n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def epsilonDecay (self, frames):\n",
    "        t = min(frames/self.epsilon_decay_duration, 1)\n",
    "        self.epsilon = self.epsilon_start*(1-t) + self.epsilon_min*t\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)\n",
    "\n",
    "def evaluate(dqn_agent, frames):\n",
    "\tevprogress = {'rewards': [], 'frames': []}\n",
    "\tif os.path.exists('eval.pickle'):\n",
    "\t\twith open('eval.pickle', 'rb') as handle:\n",
    "\t\t\tevprogress = pickle.load(handle)\n",
    "\tdqn_agent.epsilon = 0.05\n",
    "\n",
    "\tevid = len(evprogress['frames'])\n",
    "\n",
    "\tenv = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\tenv = wrappers.Monitor(env, \"vids/\"+str(evid), force=True)\n",
    "\n",
    "\ttrials  = 1\n",
    "\ttrial_len = 10000\n",
    "\ttotal_reward = 0\n",
    "\n",
    "\tfor trial in range(trials):\n",
    "\t\tcur_state = env.reset()\n",
    "\t\tcur_state = cur_state[np.newaxis,:]\n",
    "\t\ttrial_reward = 0\n",
    "\t\tfor step in range(trial_len):\n",
    "\t\t\taction = dqn_agent.act(cur_state)\n",
    "\t\t\tnew_state, reward, done, _ = env.step(action) \n",
    "\n",
    "\t\t\tnew_state = new_state[np.newaxis,:]\n",
    "\t\t\treward = clip_reward(reward)\n",
    "\n",
    "\t\t\ttrial_reward += reward\n",
    "\n",
    "\t\t\tcur_state = new_state\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\ttotal_reward += trial_reward\n",
    "\tprint(\"evaluated\",evid)\n",
    "\tprint(\"reward\",total_reward/trials)\n",
    "\tprint(\"\")\n",
    "\tevprogress['frames'].append(frames)\n",
    "\tevprogress['rewards'].append(total_reward/trials)\n",
    "\twith open('eval.pickle', 'wb') as handle:\n",
    "\t\tpickle.dump(evprogress, handle)\n",
    "\n",
    "\tdqn_agent.epsilonDecay(frames)\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"Breakout-ram-v0\")\n",
    "    trials  = 50000\n",
    "    trial_len = 10000\n",
    "    train_target_length = 10000\n",
    "    UPDATE_FREQ = 4\n",
    "    frames = 0\n",
    "    maxreward = 0\n",
    "    total_reward = 0\n",
    "    dqn_agent = DQN(env=env)\n",
    "    if os.path.exists('dqn.model'):\n",
    "      dqn_agent.model.load_weights('dqn.model')\n",
    "      dqn_agent.target_model.load_weights('dqn.model')\n",
    "    progress = {'time': [], 'rewards': [], 'frames': [], 'epsilon': [], 'losses': [], 'Qs': [], 'maxQs': [], 'total_frames': 0}\n",
    "    if os.path.exists('dqn.pickle'):\n",
    "      with open('dqn.pickle', 'rb') as handle:\n",
    "        progress = pickle.load(handle)\n",
    "        frames = progress['total_frames']\n",
    "        dqn_agent.epsilonDecay(frames)\n",
    "    for trial in range(trials):\n",
    "        cur_state = env.reset()\n",
    "        cur_state = cur_state[np.newaxis,:]\n",
    "        trial_reward = 0\n",
    "        start = timer()\n",
    "        trial_frames = 0\n",
    "\n",
    "        weight_updates = 0\n",
    "        total_losses = 0\n",
    "        total_maxQs = 0\n",
    "        total_Qs = 0\n",
    "\n",
    "        for step in range(trial_len):\n",
    "            frames += 1\n",
    "            trial_frames += 1\n",
    "\n",
    "            action = dqn_agent.act(cur_state)\n",
    "            #env.render()\n",
    "            new_state, reward, done, _ = env.step(action) \n",
    "\n",
    "            new_state = new_state[np.newaxis,:]\n",
    "            reward = clip_reward(reward)\n",
    "\n",
    "            trial_reward += reward\n",
    "            dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            if frames%UPDATE_FREQ == 0 and dqn_agent.memory.canLearn():\n",
    "              loss, maxQs, Qs = dqn_agent.replay()\n",
    "              weight_updates += 1\n",
    "              total_losses += loss\n",
    "              total_maxQs += maxQs\n",
    "              total_Qs += Qs\n",
    "            if frames%train_target_length == 0:  \n",
    "              dqn_agent.target_train()\n",
    "            dqn_agent.epsilonDecay(frames)\n",
    "            cur_state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        end = timer()\n",
    "\n",
    "        maxreward = max(maxreward, trial_reward)\n",
    "        total_reward += trial_reward\n",
    "        print(\"ran\",trial)\n",
    "        print(\"trial time\",end - start)\n",
    "        print(\"reward\",trial_reward)\n",
    "        print(\"epsilon\",dqn_agent.epsilon)\n",
    "        print(\"frames\",frames)\n",
    "        print(\"max reward\",maxreward)\n",
    "        print(\"avg reward\",total_reward/(trial+1))\n",
    "        progress['time'].append(end - start)\n",
    "        progress['rewards'].append(trial_reward)\n",
    "        progress['frames'].append(trial_frames)\n",
    "        progress['total_frames'] = frames\n",
    "        progress['epsilon'].append(dqn_agent.epsilon)\n",
    "\n",
    "        progress['losses'].append(total_losses/weight_updates)\n",
    "        progress['maxQs'].append(total_maxQs/weight_updates)\n",
    "        progress['Qs'].append(total_Qs/weight_updates)\n",
    "\n",
    "        with open('dqn.pickle', 'wb') as handle:\n",
    "          pickle.dump(progress, handle)\n",
    "        dqn_agent.save_model(\"dqn.model\")\n",
    "\n",
    "        if trial%1000 == 0:\n",
    "        \tevaluate(dqn_agent, frames)\n",
    "    return\n",
    "\n",
    "def clip_reward(reward):\n",
    "  if reward > 0:\n",
    "    return 1\n",
    "  elif reward == 0:\n",
    "   return 0\n",
    "  else:\n",
    "    return -1\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
